"""
STT (Speech-to-Text) ì„œë¹„ìŠ¤
Google Cloud Speech-to-Text + OpenAI Whisper API ì§€ì›
"""

from openai import OpenAI
from google.cloud import speech
from google.api_core import retry
from app.config import settings
import logging
import time
import tempfile
import os
import asyncio
import io

logger = logging.getLogger(__name__)


class STTService:
    """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì„œë¹„ìŠ¤ (Google Cloud & OpenAI ì§€ì›)"""
    
    def __init__(self):
        # STT ì œê³µì ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°, ê¸°ë³¸ê°’: google)
        self.provider = getattr(settings, 'STT_PROVIDER', 'google').lower()
        
        if self.provider == "google":
            self._init_google_stt()
        else:  # openai
            self._init_openai_whisper()
        
        logger.info(f"ğŸ¤ STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {self.provider.upper()}")
    
    def _init_google_stt(self):
        """Google Cloud STT ì´ˆê¸°í™”"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¸ì¦ ì •ë³´ ì„¤ì •
            credentials_path = getattr(settings, 'GOOGLE_APPLICATION_CREDENTIALS', 'credentials/google-cloud-stt.json')
            if os.path.exists(credentials_path):
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
                logger.info(f"âœ… Google Cloud ì¸ì¦ íŒŒì¼ ë¡œë“œ: {credentials_path}")
            
            self.google_client = speech.SpeechClient()
            
            # ê¸°ë³¸ ì¸ì‹ ì„¤ì •
            self.google_config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=8000,
                language_code="ko-KR",
                model="latest_short",  # ì „í™” í†µí™” ìµœì í™”
                enable_automatic_punctuation=True,
                use_enhanced=True,
                audio_channel_count=1,
            )
            
            logger.info("âœ… Google Cloud STT ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Google Cloud STT ì´ˆê¸°í™” ì‹¤íŒ¨, OpenAIë¡œ í´ë°±: {e}")
            self.provider = "openai"
            self._init_openai_whisper()
    
    def _init_openai_whisper(self):
        """OpenAI Whisper ì´ˆê¸°í™”"""
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = "whisper-1"
        self.min_chunk_size = 8000 * 2 * 0.5  # 8kHz, 16bit, ìµœì†Œ 0.5ì´ˆ
        logger.info("âœ… OpenAI Whisper ì´ˆê¸°í™” ì™„ë£Œ")
    
    def transcribe_audio(self, audio_file_path: str, language: str = "ko"):
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            audio_file_path: ìŒì„± íŒŒì¼ ê²½ë¡œ (local or URL)
            language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: ko - í•œêµ­ì–´)
        
        Returns:
            tuple: (ë³€í™˜ëœ í…ìŠ¤íŠ¸, ì‹¤í–‰ ì‹œê°„)
        """
        try:
            start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            logger.info(f"ğŸ¤ STT ë³€í™˜ ì‹œì‘: {audio_file_path}")
            
            with open(audio_file_path, "rb") as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model=self.model,
                    file=audio_file,
                    language=language,
                    response_format="text",
                    temperature=0.0,  # ëœë¤ì„± ìµœì†Œí™”
                    prompt="ì´ ì…ë ¥ì€ ì „í™” ëŒ€í™”ì˜ í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ë§ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."
                )
            
            elapsed_time = time.time() - start_time  # ì†Œìš” ì‹œê°„ ê³„ì‚°
            logger.info(f"âœ… STT ë³€í™˜ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“ ë³€í™˜ ê²°ê³¼: {transcript[:100]}...")
            
            return transcript, elapsed_time
        except Exception as e:
            logger.error(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    async def transcribe_audio_chunk(self, audio_chunk: bytes, language: str = "ko"):
        """
        ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ (ë¹„ë™ê¸° ì²˜ë¦¬)
        
        ì œê³µìì— ë”°ë¼ Google Cloud ë˜ëŠ” OpenAI Whisper ì‚¬ìš©
        
        Args:
            audio_chunk: ì˜¤ë””ì˜¤ ë°ì´í„° ì²­í¬ (ë°”ì´íŠ¸ í˜•ì‹, WAV ê¶Œì¥)
            language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: "ko" - í•œêµ­ì–´)
        
        Returns:
            tuple: (ë³€í™˜ëœ í…ìŠ¤íŠ¸, ì‹¤í–‰ ì‹œê°„)
        """
        if self.provider == "google":
            return await self._transcribe_google(audio_chunk, language)
        else:
            return await self._transcribe_openai(audio_chunk, language)
    
    async def _transcribe_google(self, audio_chunk: bytes, language: str = "ko"):
        """Google Cloud STTë¡œ ë³€í™˜"""
        try:
            start_time = time.time()
            
            # ì²­í¬ í¬ê¸° ê²€ì¦
            min_size = 8000 * 2 * 0.3  # ìµœì†Œ 0.3ì´ˆ
            if len(audio_chunk) < min_size:
                logger.debug(f"â­ï¸  ì²­í¬ê°€ ë„ˆë¬´ ì‘ì•„ ê±´ë„ˆëœ€: {len(audio_chunk)} bytes")
                return "", 0
            
            # WAV í—¤ë” ì œê±° (Googleì€ raw PCMë§Œ í•„ìš”)
            if audio_chunk[:4] == b'RIFF':
                import wave
                wav_io = io.BytesIO(audio_chunk)
                with wave.open(wav_io, 'rb') as wav_file:
                    audio_data = wav_file.readframes(wav_file.getnframes())
            else:
                audio_data = audio_chunk
            
            # Google Cloud Speech API í˜¸ì¶œ
            audio = speech.RecognitionAudio(content=audio_data)
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.google_client.recognize(
                    config=self.google_config,
                    audio=audio,
                    retry=retry.Retry(deadline=10.0)
                )
            )
            
            elapsed_time = time.time() - start_time
            
            if not response.results:
                logger.debug("â­ï¸  STT ê²°ê³¼ ì—†ìŒ")
                return "", elapsed_time
            
            transcript = response.results[0].alternatives[0].transcript
            confidence = response.results[0].alternatives[0].confidence
            
            if transcript and transcript.strip():
                logger.info(f"ğŸ¤ [Google STT] {transcript[:80]}... "
                           f"(ì‹ ë¢°ë„: {confidence:.2f}, {elapsed_time:.2f}ì´ˆ)")
            
            return transcript, elapsed_time
            
        except Exception as e:
            logger.error(f"âŒ Google STT ë³€í™˜ ì‹¤íŒ¨: {e}")
            return "", 0
    
    async def _transcribe_openai(self, audio_chunk: bytes, language: str = "ko"):
        """OpenAI Whisperë¡œ ë³€í™˜"""
        try:
            start_time = time.time()
            
            # ì²­í¬ í¬ê¸° ê²€ì¦
            if len(audio_chunk) < self.min_chunk_size:
                logger.debug(f"â­ï¸  ì²­í¬ê°€ ë„ˆë¬´ ì‘ì•„ ê±´ë„ˆëœ€: {len(audio_chunk)} bytes")
                return "", 0
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name
                temp_file.write(audio_chunk)
            
            try:
                loop = asyncio.get_event_loop()
                transcript = await loop.run_in_executor(
                    None,
                    self._transcribe_file_sync,
                    temp_path,
                    language
                )
                
                elapsed_time = time.time() - start_time
                
                if transcript and transcript.strip():
                    logger.info(f"ğŸ¤ [OpenAI STT] {transcript[:80]}... ({elapsed_time:.2f}ì´ˆ)")
                
                return transcript, elapsed_time
                
            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            logger.error(f"âŒ OpenAI STT ë³€í™˜ ì‹¤íŒ¨: {e}")
            return "", 0
    
    def _transcribe_file_sync(self, file_path: str, language: str) -> str:
        """
        ë™ê¸° ë°©ì‹ íŒŒì¼ ë³€í™˜ (executorì—ì„œ ì‹¤í–‰ìš©)
        
        ì´ ë©”ì„œë“œëŠ” ì§ì ‘ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
        transcribe_audio_chunk()ì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        
        Args:
            file_path: ë³€í™˜í•  ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ
        
        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        with open(file_path, "rb") as audio_file:
            transcript = self.client.audio.transcriptions.create(
                model=self.model,
                file=audio_file,
                language=language,
                response_format="text",
                temperature=0.0,  # ëœë¤ì„± ìµœì†Œí™”
                prompt="ì´ ì…ë ¥ì€ ì „í™” ëŒ€í™”ì˜ í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ë§ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."
            )
        return transcript
    
    def transcribe_audio_with_timestamps(self, audio_file_path: str):
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ë³€í™˜
        
        Args:
            audio_file_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
        
        Returns:
            dict: segmentsì™€ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´
        """
        try:
            with open(audio_file_path, "rb") as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model=self.model,
                    file=audio_file,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"],
                    temperature=0.0
                )
            return transcript
        except Exception as e:
            logger.error(f"Failed to transcribe with timestamps: {e}")
            raise

