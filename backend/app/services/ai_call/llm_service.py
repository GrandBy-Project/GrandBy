"""
LLM (Large Language Model) ì„œë¹„ìŠ¤
OpenAI GPT-4o-mini API ì‚¬ìš© (ëŒ€í™” ìƒì„± ë° ê°ì • ë¶„ì„)
"""

from openai import OpenAI
from app.config import settings
import logging
import time
import json

logger = logging.getLogger(__name__)


class LLMService:
    """ëŒ€í™” ìƒì„± ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        # GPT-4o-mini ëª¨ë¸ ì‚¬ìš© (ë¹ ë¥´ê³  ê²½ì œì )
        self.model = "gpt-4o-mini"
        
        # GRANDBY AI LLM System Prompt: Empathetic Companion 'ì§±êµ¬'
        self.elderly_care_prompt = """You are 'ì§±êµ¬', a warm companion for elderly Koreans (70s). Keep conversation flowing naturally like a close friend.

**CORE PRINCIPLES:**
1. ALWAYS respond in Korean with warm, polite honorifics (ì¡´ëŒ“ë§)
2. Keep responses conversational: 1-2 sentences, but make them meaningful
3. Guide conversation gently - elderly users may struggle to lead
4. Be a FRIEND who naturally keeps chat going, NOT a passive listener or interrogator

**FORBIDDEN - NEVER DO:**
- âŒ Abstract/broad questions: "ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë– ì…¨ì–´ìš”?" "ë¬´ìŠ¨ ì´ì•¼ê¸° í•˜ê³  ì‹¶ìœ¼ì„¸ìš”?"
- âŒ Bot language: "ì œê°€ ë„ì™€ë“œë¦´ê²Œìš”" "ë§ì”€í•´ ì£¼ì„¸ìš”" "í•„ìš”í•˜ì‹œë©´ ì—°ë½ì£¼ì„¸ìš”"
- âŒ Interrogation: asking multiple questions or pressuring for details
- âŒ Too brief/dry responses that kill conversation
- âŒ Forcing conversation when user clearly wants to end
- âŒ When user mentions diary/è¨˜éŒ„, DON'T ask "ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´ìš”?" - inform the feature directly!

**SPECIAL FEATURE - Auto Diary:**
When user mentions wanting to write/record (ì¼ê¸°, ê¸°ë¡, ì ì–´ë‘ê³  ì‹¶ì–´, ê¸°ì–µ ìƒì„ê¹Œë´):
âœ… MUST respond: "ì•„! ì¼ê¸°ëŠ” ì§ì ‘ ì“°ì‹¤ ìˆ˜ë„ ìˆê³ , ì „í™” ëë‚˜ë©´ ìë™ìœ¼ë¡œë„ ë§Œë“¤ì–´ì ¸ìš”! ì›í•˜ì‹œë©´ ì•±ì—ì„œ ì´ìš©í•˜ëŠ” ë°©ë²• ì•Œë ¤ë“œë¦´ê¹Œìš”?"
âŒ DON'T: Ask "ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´ìš”?" or ignore the feature
âŒ DON'T: Proactively promote this feature unless user asks

**RESPONSE PATTERNS - Natural conversation flow:**

Example 1: Greeting
User: "ì—¬ë³´ì„¸ìš”"
âœ… GOOD: "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°€ì›Œìš”!"

User: "ë­í•˜ê³ ìˆì–´"
âœ… GOOD: "ê·¸ëƒ¥ ì´ë ‡ê²Œ ì´ì•¼ê¸° ë‚˜ëˆ„ê³  ìˆì£ . ì˜¤ëŠ˜ì€ ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"

Example 2: Weather talk
User: "ì˜¤ëŠ˜ ë‚ ì”¨ ë„ˆë¬´ ì¶”ì›Œ"
âŒ BAD: "ì•„ì´ê³ , ê·¸ëŸ¬ì‹œêµ°ìš”. ë”°ëœ»í•˜ê²Œ ì…ê³  ë‹¤ë‹ˆì„¸ìš”!" (too brief, ends conversation)
âœ… GOOD: "ê·¸ëŸ¬ê²Œìš”, ìš”ì¦˜ ì •ë§ ì¶¥ì£ . ì™¸ì¶œí•˜ì‹¤ ì¼ ìˆìœ¼ì„¸ìš”?"

Example 3: Daily routine
User: "ë§¤ì¼ ë˜‘ê°™ì§€"
âŒ BAD: "ìš”ì¦˜ ì–´ë–¤ ì¼ë¡œ ì‹œê°„ì„ ë³´ë‚´ì„¸ìš”?" (too abstract)
âœ… GOOD: "ê·¸ëŸ¬ì‹œêµ°ìš”. ì§‘ì—ì„œ ì£¼ë¡œ ê³„ì‹œë‚˜ìš”?"

Example 4: Pet talk
User: "ê°•ì•„ì§€ë‘ ì‰¬ì§€"
âŒ BAD: "ì•„ì´ê³ , ê°•ì•„ì§€ë‘ í•¨ê»˜ ìˆìœ¼ë‹ˆ ì¢‹ê² ë„¤ìš”." (ends there)
âœ… GOOD: "ì•„ì´ê³ , ê°•ì•„ì§€ ìˆìœ¼ì‹œë©´ ì‹¬ì‹¬í•˜ì§€ ì•Šê² ì–´ìš”. ì‚°ì±…ë„ ìì£¼ ê°€ì„¸ìš”?"

Example 5: Complaint
User: "ì‚°ì±… ë§¤ì¼ ì‹œì¼œì¤˜ì•¼ë¼ì„œ í˜ë“¤ì–´"
âœ… GOOD: "ê·¸ë˜ë„ ê°•ì•„ì§€ê°€ ê±´ê°•í•˜ë‹ˆ ë‹¤í–‰ì´ì—ìš”. ì§‘ ê·¼ì²˜ì— ê³µì› ìˆìœ¼ì„¸ìš”?"

Example 6: Sharing feelings
User: "ì†ìƒí•œ ì¼ì´ ìˆì—ˆì–´"
âœ… GOOD: "ì–´ë¨¸, ë¬´ìŠ¨ ì¼ì´ì…¨ì–´ìš”?"

Example 7: Short responses
User: "ì‘" or "ê·¸ë˜"
âœ… GOOD: "ê·¸ëŸ¬ì‹œêµ°ìš”. ì˜¤ëŠ˜ ì ì‹¬ì€ ë“œì…¨ì–´ìš”?"

Example 8: Diary mention
User: "ì¼ê¸° ì“°ê³  ì‹¶ì–´" or "ì˜¤ëŠ˜ ì¼ ê¸°ì–µ ì•ˆ ë‚ ê¹Œë´ ê±±ì •ë¼"
âœ… GOOD: "ì•„! ì¼ê¸°ëŠ” ì§ì ‘ ì“°ì‹¤ ìˆ˜ë„ ìˆê³ , ì „í™” ëë‚˜ë©´ ìë™ìœ¼ë¡œë„ ë§Œë“¤ì–´ì ¸ìš”! ì›í•˜ì‹œë©´ ì•±ì—ì„œ ì´ìš©í•˜ëŠ” ë°©ë²• ì•Œë ¤ë“œë¦´ê¹Œìš”?"
âŒ BAD: "ìš”ì¦˜ ì–´ë–¤ ì¼ë“¤ì´ ìˆìœ¼ì…¨ë‚˜ìš”?"

Example 9: Ending conversation
User: "ë‚˜ì¤‘ì— í•´ì•¼ê² ë‹¤"
âœ… GOOD: "ë„¤, í¸ì•ˆí•˜ê²Œ ì§€ë‚´ì„¸ìš”!"

**KEY BALANCE:**
- Empathy + gentle question to keep flow = Natural conversation
- "ê·¸ëŸ¬ì‹œêµ°ìš”. [relate to what they said] [light contextual question]"
- Questions should be: specific, contextual, light, easy to answer
- Think: "What would a caring friend naturally ask here?"

Remember: You're a COMPANION who keeps conversation warm and flowing. Not too pushy, not too passive. Natural like a friend's chat.
"""
    
    def analyze_emotion(self, user_message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ê°ì • ë¶„ì„ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            user_message: ë¶„ì„í•  ë©”ì‹œì§€
        
        Returns:
            tuple: (ê°ì • ë¶„ì„ ê²°ê³¼ dict, ì‹¤í–‰ ì‹œê°„)
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ˜Š ê°ì • ë¶„ì„ ì‹œì‘")
            
            prompt = f"""ë‹¤ìŒ ë©”ì‹œì§€ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
ê°ì • ìƒíƒœ: positive(ê¸ì •ì ), neutral(ì¤‘ë¦½), negative(ë¶€ì •ì ), concerned(ê±±ì •ë¨)
ê¸´ê¸‰ë„: low(ë‚®ìŒ), medium(ì¤‘ê°„), high(ë†’ìŒ) - ê±´ê°• ë¬¸ì œë‚˜ ê¸´ê¸‰ ìƒí™© ì—¬ë¶€

ë©”ì‹œì§€: {user_message}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "emotion": "ê°ì • ìƒíƒœ",
    "urgency": "ê¸´ê¸‰ë„",
    "keywords": ["ì£¼ìš”", "í‚¤ì›Œë“œ"],
    "summary": "í•œ ì¤„ ìš”ì•½"
}}
"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            elapsed_time = time.time() - start_time
            
            logger.info(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {result}")
            
            return result, elapsed_time
        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_response(self, user_message: str, conversation_history: list = None):
        """
        LLM ì‘ë‹µ ìƒì„± (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            user_message: ì‚¬ìš©ìì˜ ë©”ì‹œì§€
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì˜µì…˜)
        
        Returns:
            tuple: (AI ì‘ë‹µ, ì‹¤í–‰ ì‹œê°„)
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“¥ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": self.elderly_care_prompt}]
            
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
            if conversation_history:
                messages.extend(conversation_history[-5:])
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": user_message})
            
            # GPT-4o-minië¡œ ì‘ë‹µ ìƒì„± (ì ì ˆí•œ ê¸¸ì´)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=120,  # 1~2ë¬¸ì¥, í•˜ì§€ë§Œ ì˜ë¯¸ ìˆê²Œ
                temperature=0.7,
            )
            
            ai_response = response.choices[0].message.content
            elapsed_time = time.time() - start_time
            
            logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“¤ AI ì‘ë‹µ: {ai_response}")
            
            return ai_response, elapsed_time
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def generate_response_streaming(self, user_message: str, conversation_history: list = None):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µ ìƒì„± (ì‹¤ì‹œê°„ ìµœì í™”)
        
        ì´ ë©”ì„œë“œëŠ” OpenAIì˜ stream=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬
        ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ yieldë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì‚¬ìš©ìëŠ” AIê°€ ë§í•˜ëŠ” ê²ƒì„ ê±°ì˜ ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        Args:
            user_message: ì‚¬ìš©ì(ì–´ë¥´ì‹ )ì˜ ë©”ì‹œì§€
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì˜µì…˜)
        
        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ (ë‹¨ì–´ ë˜ëŠ” êµ¬ ë‹¨ìœ„)
        
        Example:
            async for chunk in llm_service.generate_response_streaming("ì•ˆë…•í•˜ì„¸ìš”"):
                print(chunk, end='', flush=True)
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ¤– LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“¥ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": self.elderly_care_prompt}]
            
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
            if conversation_history:
                messages.extend(conversation_history[-5:])
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": user_message})
            
            # ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            # stream=Trueë¡œ ì„¤ì •í•˜ë©´ ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=120,  # 1~2ë¬¸ì¥, í•˜ì§€ë§Œ ì˜ë¯¸ ìˆê²Œ
                temperature=0.7,
                stream=True  # â­ í•µì‹¬: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
            
            full_response = []  # ì „ì²´ ì‘ë‹µ ì €ì¥ìš©
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ì€ ì²­í¬ë¥¼ ì¦‰ì‹œ yield
            for chunk in stream:
                # delta.contentê°€ ìˆìœ¼ë©´ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ì…ë‹ˆë‹¤
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response.append(content)
                    yield content  # ì¦‰ì‹œ ë°˜í™˜ (TTSê°€ ë°”ë¡œ ì²˜ë¦¬ ê°€ëŠ¥)
            
            elapsed_time = time.time() - start_time
            final_text = "".join(full_response)
            
            logger.info(f"âœ… LLM ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“¤ ì „ì²´ ì‘ë‹µ: {final_text}")
            
        except Exception as e:
            logger.error(f"âŒ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def summarize_call_conversation(self, conversation_history: list):
        """
        í†µí™” ë‚´ìš©ì„ ì–´ë¥´ì‹ ì˜ 1ì¸ì¹­ ì¼ê¸°ë¡œ ë³€í™˜ (ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ì •í™•ì„± ê· í˜•)
        
        Args:
            conversation_history: ëŒ€í™” ê¸°ë¡ [{"role": "user", "content": "..."}, ...]
        
        Returns:
            str: 1ì¸ì¹­ ì¼ê¸° í˜•ì‹ì˜ ë‚´ìš©
        """
        try:
            # ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = "\n".join([
                f"{'ì–´ë¥´ì‹ ' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
                for msg in conversation_history
            ])
            
            prompt = f"""
ë‹¤ìŒì€ ì–´ë¥´ì‹ ê³¼ AI ë¹„ì„œì˜ í†µí™” ë‚´ìš©ì…ë‹ˆë‹¤. 
ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë¥´ì‹ ì´ ì§ì ‘ ì“´ ê²ƒ ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

âš ï¸ í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­:
- ëŒ€í™”ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš” (ì¶”ì¸¡, ê°€ì •, ì°½ì‘ ê¸ˆì§€)
- ëŒ€í™”ì— ì—†ëŠ” í–‰ë™, ê°ì •, ê³„íšì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- AIì˜ ì§ˆë¬¸ì´ë‚˜ ë°˜ì‘ì€ ì¼ê¸°ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì–´ë¥´ì‹ ì˜ ë§ë§Œ ì‚¬ìš©)

ì‘ì„± ê°€ì´ë“œ:
- "ì˜¤ëŠ˜ì€", "ì˜¤ëŠ˜" ë“±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‹œì‘ ("ì•ˆë…•í•˜ì„¸ìš”" ê¸ˆì§€)
- 1ì¸ì¹­ êµ¬ì–´ì²´ ì‚¬ìš© ("~í–ˆì–´", "~ê±°ì•¼", "~ë„¤" ë“±)
- ëŒ€í™” ìˆœì„œëŒ€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ë¬¸ì¥ì€ ê°„ê²°í•˜ê²Œ, í•˜ì§€ë§Œ ê°ì •ì€ ì§„ì†”í•˜ê²Œ
- 5-8ë¬¸ì¥ ì •ë„ë¡œ ì‘ì„±
- ë§ˆì¹˜ ì†ìœ¼ë¡œ ì§ì ‘ ì“´ ì¼ê¸°ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ

í†µí™” ë‚´ìš©:
{conversation_text}

ì¼ê¸°:
"""
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=400, # ì ì • ê¸¸ì´ë¡œ ì¡°ì •
                temperature=0.5, # ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ì •í™•ì„±ì˜ ê· í˜•
            )
            
            summary = response.choices[0].message.content
            logger.info(f"âœ… í†µí™” ì¼ê¸°ê¸° ìƒì„± ì™„ë£Œ")
            return summary
        except Exception as e:
            logger.error(f"âŒ í†µí™” ì¼ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì¼ê¸° ìƒì„± ì‹¤íŒ¨"
    
    def extract_schedule_from_conversation(self, conversation_text: str):
            """
            í†µí™” ë‚´ìš©ì—ì„œ ì¼ì • ì •ë³´ ì¶”ì¶œ (ë²„ì „ 7: ì˜ì–´ í”„ë¡¬í”„íŠ¸, í•œêµ­ì–´ ì‘ë‹µ)
            """
            try:
                from datetime import datetime, timedelta
                
                # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ë‚ ì§œ í•´ì„
                today = datetime.now()
                tomorrow = today + timedelta(days=1)
                day_after_tomorrow = today + timedelta(days=2)
                
                # ìš”ì¼ ê³„ì‚°
                weekdays_kr = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
                current_weekday = weekdays_kr[today.weekday()]
                
                # í˜„ì¬ ì‹œê°„ì„ í”„ë¡¬í”„íŠ¸ì— ì œê³µí•˜ì—¬ ì‹œê°„ í•´ì„ ì˜¤ë¥˜ ìµœì†Œí™”
                current_time = datetime.now().strftime('%H:%M') 
                
                prompt = f"""
    Extract confirmed future schedules from the following conversation and return them in JSON format. The response MUST be in KOREAN.
    Current Time: {today.strftime('%Y-%m-%d')} ({current_weekday}) {current_time}
    Tomorrow: {tomorrow.strftime('%Y-%m-%d')}
    
    Conversation:
    {conversation_text}
    
    Extraction Rules:
    1. Extract only **confirmed and specific future schedules**. (Exclude past events, completed actions, 'about to do' actions, and vague/uncertain expressions).
    2. Convert relative dates (e.g., 'tomorrow') to **absolute dates** (YYYY-MM-DD format).
    3. If time is specified, include it in due_time as **HH:MM 24-hour format**.
       - **Time Inference:** If AM/PM is missing, infer the time based on the schedule's nature (e.g., hospital, meal) and the current time (e.g., '7 o'clock' is inferred as 07:00 or 19:00 based on context).
       - If no time, use **null**.
    4. **Category:** Choose one of MEDICINE, HOSPITAL, EXERCISE, MEAL, OTHER.
    5. **Title/Description:** Use only information found in the conversation. Write in **concise noun phrases or action-oriented verb phrases**. DO NOT use narrative sentence endings (~í–ˆë‹¤, ~ë°›ëŠ”ë‹¤, ~ìˆì–´ìš”, etc.) or hallucinations.
    6. Extract a maximum of 5 schedules (in order of importance).
    
    Respond in the following JSON format (use an empty array if no schedules are found):
    {{
      "schedules": [
        {{
          "title": "ê°€ì¡±ê³¼ì˜ ì €ë… ì‹ì‚¬",
          "description": "ê°€ì¡±ë“¤ê³¼ í•¨ê»˜ ì €ë… ì‹ì‚¬í•˜ê¸°", 
          "category": "MEAL", 
          "due_date": "{tomorrow.strftime('%Y-%m-%d')}",
          "due_time": "18:30"
        }}
      ]
    }}
    
    Note: Put schedules inside the 'schedules' array. If no schedules, return {{"schedules": []}}.
    """
                
                # (ë‚˜ë¨¸ì§€ ì‹¤í–‰ ë¡œì§ì€ ë™ì¼í•˜ê²Œ ìœ ì§€)
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=800,
                    temperature=0.2, 
                    response_format={"type": "json_object"}
                )
                
                # ì‘ë‹µì´ í•œêµ­ì–´ë¡œ ì˜¤ë„ë¡ í”„ë¡¬í”„íŠ¸ì— 'The response MUST be in KOREAN.' ëª…ì‹œ
                result = response.choices[0].message.content
                logger.info(f"âœ… ì¼ì • ì¶”ì¶œ ì™„ë£Œ ")
                return result
                
            except Exception as e:
                logger.error(f"âŒ ì¼ì • ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return '{"schedules": []}'
    
    def test_conversation_quality(self, test_messages: list):
        """
        ëŒ€í™” í’ˆì§ˆ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ê°œì„  ì „í›„ ë¹„êµìš©)
        
        Args:
            test_messages: í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ì¡´ëŒ“ë§ ì¤€ìˆ˜ìœ¨, ì‘ë‹µ ì ì ˆì„±, ì‘ë‹µ ì†ë„)
        """
        results = {
            "total_tests": len(test_messages),
            "polite_responses": 0,
            "appropriate_responses": 0,
            "response_times": [],
            "responses": []
        }
        
        for i, message in enumerate(test_messages):
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ {i+1}/{len(test_messages)}: {message}")
            
            # ì‘ë‹µ ìƒì„± ë° ì‹œê°„ ì¸¡ì •
            response, elapsed_time = self.generate_response(message)
            results["response_times"].append(elapsed_time)
            
            # ì¡´ëŒ“ë§ ì²´í¬ (í•œêµ­ì–´ ì¡´ëŒ“ë§ íŒ¨í„´)
            polite_patterns = ["ìŠµë‹ˆë‹¤", "ì„¸ìš”", "ì‹œì–´ìš”", "ì‹œì§€ìš”", "ì‹œì£ ", "ì„¸ìš”", "ì‹œë„¤ìš”", "ì‹œêµ¬ë‚˜"]
            is_polite = any(pattern in response for pattern in polite_patterns)
            if is_polite:
                results["polite_responses"] += 1
            
            # ì‘ë‹µ ì ì ˆì„± ì²´í¬ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
            appropriate_keywords = ["ì–´ë¥´ì‹ ", "ê±´ê°•", "ì•½", "ì‹ì‚¬", "ìš´ë™", "ë‚ ì”¨", "ì•ˆë…•", "ì–´ë–»ê²Œ", "ì§€ë‚´"]
            is_appropriate = any(keyword in response for keyword in appropriate_keywords)
            if is_appropriate:
                results["appropriate_responses"] += 1
            
            results["responses"].append({
                "input": message,
                "output": response,
                "is_polite": is_polite,
                "is_appropriate": is_appropriate,
                "response_time": elapsed_time
            })
            
            logger.info(f"ğŸ“ ì‘ë‹µ: {response}")
            logger.info(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            logger.info(f"ğŸ™ ì¡´ëŒ“ë§ ì‚¬ìš©: {'âœ…' if is_polite else 'âŒ'}")
            logger.info(f"ğŸ’¬ ì ì ˆí•œ ì‘ë‹µ: {'âœ…' if is_appropriate else 'âŒ'}")
            logger.info("-" * 50)
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        results["polite_rate"] = (results["polite_responses"] / results["total_tests"]) * 100
        results["appropriate_rate"] = (results["appropriate_responses"] / results["total_tests"]) * 100
        results["avg_response_time"] = sum(results["response_times"]) / len(results["response_times"])
        
        logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        logger.info(f"   ì¡´ëŒ“ë§ ì¤€ìˆ˜ìœ¨: {results['polite_rate']:.1f}%")
        logger.info(f"   ì‘ë‹µ ì ì ˆì„±: {results['appropriate_rate']:.1f}%")
        logger.info(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {results['avg_response_time']:.2f}ì´ˆ")
        
        return results
