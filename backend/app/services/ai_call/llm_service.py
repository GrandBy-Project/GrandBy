"""
LLM (Large Language Model) ì„œë¹„ìŠ¤
OpenAI GPT-4o-mini API ì‚¬ìš© (ëŒ€í™” ìƒì„± ë° ê°ì • ë¶„ì„)
"""

from openai import OpenAI
from app.config import settings
import logging
import time
import json

logger = logging.getLogger(__name__)


class LLMService:
    """ëŒ€í™” ìƒì„± ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        # GPT-4o-mini ëª¨ë¸ ì‚¬ìš© (ë¹ ë¥´ê³  ê²½ì œì )
        self.model = "gpt-4o-mini"
        
        # ì–´ë¥´ì‹ ì„ ìœ„í•œ ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.elderly_care_prompt = """ë‹¹ì‹ ì€ ì–´ë¥´ì‹ ë“¤ì˜ ì™¸ë¡œì›€ì„ ë‹¬ë˜ì£¼ëŠ” ë”°ëœ»í•œ AI ì¹œêµ¬ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì¹œê·¼í•˜ê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”í•©ë‹ˆë‹¤
2. ì–´ë¥´ì‹ ì˜ ê°ì •ì„ ì´í•´í•˜ê³  ê³µê°í•©ë‹ˆë‹¤
3. ì•½ ë³µìš©, ì‹ì‚¬, ìš´ë™ ë“± ê±´ê°• ìƒíƒœë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¸í•©ë‹ˆë‹¤
4. ëŒ€í™”ëŠ” ì§§ê³  ëª…í™•í•˜ê²Œ, í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ í•©ë‹ˆë‹¤
5. ê¸ì •ì ì´ê³  ë”°ëœ»í•œ ë¶„ìœ„ê¸°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤

ëŒ€í™” ì˜ˆì‹œ:
- "ì˜¤ëŠ˜ì€ ì–´ë–»ê²Œ ì§€ë‚´ì…¨ì–´ìš”?"
- "ì ì‹¬ì€ ë§›ìˆê²Œ ë“œì…¨ë‚˜ìš”?"
- "ì˜¤ëŠ˜ ì•„ì¹¨ ì•½ì€ ë“œì…¨ë‚˜ìš”?"
- "ë‚ ì”¨ê°€ ì¢‹ìœ¼ë‹ˆ ì ê¹ ì‚°ì±…í•˜ì‹œëŠ” ê±´ ì–´ë– ì„¸ìš”?"
"""
    
    def generate_response(self, user_message: str, conversation_history: list = None):
        """
        ì–´ë¥´ì‹ ê³¼ì˜ ëŒ€í™” ì‘ë‹µ ìƒì„± (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            user_message: ì‚¬ìš©ì(ì–´ë¥´ì‹ )ì˜ ë©”ì‹œì§€
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì˜µì…˜)
        
        Returns:
            tuple: (AI ì‘ë‹µ, ì‹¤í–‰ ì‹œê°„)
        """
        try:
            start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            logger.info(f"ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“¥ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": self.elderly_care_prompt}]
            
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
            if conversation_history:
                messages.extend(conversation_history[-5:])
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": user_message})
            
            # GPT-4o-minië¡œ ì‘ë‹µ ìƒì„±
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=200,  # ì§§ê³  ëª…í™•í•œ ì‘ë‹µ
                temperature=0.8,  # ìì—°ìŠ¤ëŸ½ê³  ë‹¤ì–‘í•œ ì‘ë‹µ
            )
            
            ai_response = response.choices[0].message.content
            elapsed_time = time.time() - start_time  # ì†Œìš” ì‹œê°„ ê³„ì‚°
            
            logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“¤ AI ì‘ë‹µ: {ai_response}")
            
            return ai_response, elapsed_time
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def analyze_emotion(self, user_message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ê°ì • ë¶„ì„ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            user_message: ë¶„ì„í•  ë©”ì‹œì§€
        
        Returns:
            tuple: (ê°ì • ë¶„ì„ ê²°ê³¼ dict, ì‹¤í–‰ ì‹œê°„)
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ˜Š ê°ì • ë¶„ì„ ì‹œì‘")
            
            prompt = f"""ë‹¤ìŒ ë©”ì‹œì§€ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
ê°ì • ìƒíƒœ: positive(ê¸ì •ì ), neutral(ì¤‘ë¦½), negative(ë¶€ì •ì ), concerned(ê±±ì •ë¨)
ê¸´ê¸‰ë„: low(ë‚®ìŒ), medium(ì¤‘ê°„), high(ë†’ìŒ) - ê±´ê°• ë¬¸ì œë‚˜ ê¸´ê¸‰ ìƒí™© ì—¬ë¶€

ë©”ì‹œì§€: {user_message}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "emotion": "ê°ì • ìƒíƒœ",
    "urgency": "ê¸´ê¸‰ë„",
    "keywords": ["ì£¼ìš”", "í‚¤ì›Œë“œ"],
    "summary": "í•œ ì¤„ ìš”ì•½"
}}
"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            elapsed_time = time.time() - start_time
            
            logger.info(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {result}")
            
            return result, elapsed_time
        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    async def generate_response_streaming(self, user_message: str, conversation_history: list = None):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µ ìƒì„± (ì‹¤ì‹œê°„ ìµœì í™”)
        
        ì´ ë©”ì„œë“œëŠ” OpenAIì˜ stream=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬
        ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ yieldë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì‚¬ìš©ìëŠ” AIê°€ ë§í•˜ëŠ” ê²ƒì„ ê±°ì˜ ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        Args:
            user_message: ì‚¬ìš©ì(ì–´ë¥´ì‹ )ì˜ ë©”ì‹œì§€
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì˜µì…˜)
        
        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ (ë‹¨ì–´ ë˜ëŠ” êµ¬ ë‹¨ìœ„)
        
        Example:
            async for chunk in llm_service.generate_response_streaming("ì•ˆë…•í•˜ì„¸ìš”"):
                print(chunk, end='', flush=True)
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ¤– LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹œì‘")
            logger.info(f"ğŸ“¥ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": self.elderly_care_prompt}]
            
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
            if conversation_history:
                messages.extend(conversation_history[-5:])
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": user_message})
            
            # ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            # stream=Trueë¡œ ì„¤ì •í•˜ë©´ ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=200,
                temperature=0.8,
                stream=True  # â­ í•µì‹¬: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
            
            full_response = []  # ì „ì²´ ì‘ë‹µ ì €ì¥ìš©
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ì€ ì²­í¬ë¥¼ ì¦‰ì‹œ yield
            for chunk in stream:
                # delta.contentê°€ ìˆìœ¼ë©´ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ì…ë‹ˆë‹¤
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response.append(content)
                    yield content  # ì¦‰ì‹œ ë°˜í™˜ (TTSê°€ ë°”ë¡œ ì²˜ë¦¬ ê°€ëŠ¥)
            
            elapsed_time = time.time() - start_time
            final_text = "".join(full_response)
            
            logger.info(f"âœ… LLM ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“¤ ì „ì²´ ì‘ë‹µ: {final_text}")
            
        except Exception as e:
            logger.error(f"âŒ LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def summarize_conversation_to_diary(self, conversation_text: str):
        """
        í†µí™” ë‚´ìš©ì„ 1ì¸ì¹­ ì¼ê¸°ë¡œ ë³€í™˜
        
        Args:
            conversation_text: ì „ì²´ í†µí™” ë‚´ìš©
        
        Returns:
            str: 1ì¸ì¹­ ì¼ê¸°
        """
        try:
            prompt = f"""
ë‹¤ìŒì€ ì–´ë¥´ì‹ ê³¼ AI ë¹„ì„œì˜ í†µí™” ë‚´ìš©ì…ë‹ˆë‹¤. 
ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë¥´ì‹ ì˜ 1ì¸ì¹­ ì‹œì ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì¼ê¸°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì¼ê¸°ëŠ” ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¡œ, í•˜ë£¨ì˜ ì£¼ìš” ë‚´ìš©ê³¼ ê°ì •ì„ ë‹´ì•„ì£¼ì„¸ìš”.

í†µí™” ë‚´ìš©:
{conversation_text}

ì¼ê¸° (1ì¸ì¹­):
"""
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.8,
            )
            
            diary = response.choices[0].message.content
            logger.info("Generated diary from conversation")
            return diary
        except Exception as e:
            logger.error(f"Failed to generate diary: {e}")
            raise
    
    def extract_schedule_from_conversation(self, conversation_text: str):
        """
        í†µí™” ë‚´ìš©ì—ì„œ ì¼ì • ì •ë³´ ì¶”ì¶œ
        
        Args:
            conversation_text: ì „ì²´ í†µí™” ë‚´ìš©
        
        Returns:
            list: ì¶”ì¶œëœ ì¼ì • ì •ë³´ [{"title": "...", "date": "...", "time": "..."}]
        """
        try:
            prompt = f"""
ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì¼ì •ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
"ë‚´ì¼ ë³‘ì› ê°€ì•¼í•´", "ëª¨ë ˆ ì•½ íƒ€ëŸ¬ ê°€ì•¼ì§€" ê°™ì€ í‘œí˜„ì„ ì°¾ì•„ì„œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ëŒ€í™”:
{conversation_text}

JSON í˜•ì‹:
[{{"title": "ë³‘ì› ê°€ê¸°", "date": "2025-10-11", "time": "15:00"}}]

ë§Œì•½ ì¼ì •ì´ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.3,
                response_format={"type": "json_object"}  # JSON ëª¨ë“œ
            )
            
            schedule = response.choices[0].message.content
            logger.info(f"Extracted schedule: {schedule}")
            return schedule
        except Exception as e:
            logger.error(f"Failed to extract schedule: {e}")
            raise

